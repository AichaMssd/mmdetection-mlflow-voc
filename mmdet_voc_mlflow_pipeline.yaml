# PIPELINE DEFINITION
# Name: mmdetection-ssd-voc2007-training-with-mlflow
# Description: Train SSD300 on Pascal VOC 2007 with MLflow tracking
# Inputs:
#    batch_size: int [Default: 8.0]
#    github_repo: str [Default: '']
#    learning_rate: float [Default: 0.001]
#    mlflow_experiment_name: str [Default: 'ssd_voc_training']
#    mlflow_tracking_uri: str [Default: '']
#    num_epochs: int [Default: 24.0]
# Outputs:
#    train-ssd-with-mlflow-metrics_output: system.Metrics
components:
  comp-prepare-voc-dataset:
    executorLabel: exec-prepare-voc-dataset
    inputDefinitions:
      parameters:
        dataset_name:
          defaultValue: voc2007
          isOptional: true
          parameterType: STRING
    outputDefinitions:
      artifacts:
        output_dataset:
          artifactType:
            schemaTitle: system.Dataset
            schemaVersion: 0.0.1
  comp-train-ssd-with-mlflow:
    executorLabel: exec-train-ssd-with-mlflow
    inputDefinitions:
      artifacts:
        dataset:
          artifactType:
            schemaTitle: system.Dataset
            schemaVersion: 0.0.1
      parameters:
        batch_size:
          defaultValue: 8.0
          isOptional: true
          parameterType: NUMBER_INTEGER
        config_path:
          defaultValue: configs/pascal_voc/ssd300_voc0712.py
          isOptional: true
          parameterType: STRING
        github_repo:
          defaultValue: ''
          isOptional: true
          parameterType: STRING
        learning_rate:
          defaultValue: 0.001
          isOptional: true
          parameterType: NUMBER_DOUBLE
        mlflow_experiment_name:
          defaultValue: ssd_voc_training
          isOptional: true
          parameterType: STRING
        mlflow_tracking_uri:
          defaultValue: ''
          isOptional: true
          parameterType: STRING
        num_epochs:
          defaultValue: 24.0
          isOptional: true
          parameterType: NUMBER_INTEGER
    outputDefinitions:
      artifacts:
        config_output:
          artifactType:
            schemaTitle: system.Artifact
            schemaVersion: 0.0.1
        metrics_output:
          artifactType:
            schemaTitle: system.Metrics
            schemaVersion: 0.0.1
        model_output:
          artifactType:
            schemaTitle: system.Model
            schemaVersion: 0.0.1
deploymentSpec:
  executors:
    exec-prepare-voc-dataset:
      container:
        args:
        - --executor_input
        - '{{$}}'
        - --function_to_execute
        - prepare_voc_dataset
        command:
        - sh
        - -c
        - "\nif ! [ -x \"$(command -v pip)\" ]; then\n    python3 -m ensurepip ||\
          \ python3 -m ensurepip --user || apt-get install python3-pip\nfi\n\nPIP_DISABLE_PIP_VERSION_CHECK=1\
          \ python3 -m pip install --quiet --no-warn-script-location 'kfp==2.9.0'\
          \ '--no-deps' 'typing-extensions>=3.7.4,<5; python_version<\"3.9\"'  &&\
          \  python3 -m pip install --quiet --no-warn-script-location 'openmim==0.3.9'\
          \ 'opencv-python==4.11.0.86' 'pycocotools==2.0.7' 'lxml' 'tqdm==4.65.2'\
          \ 'requests==2.28.2' 'mmengine==0.10.7' 'mmcv==2.0.0' && \"$0\" \"$@\"\n"
        - sh
        - -ec
        - 'program_path=$(mktemp -d)


          printf "%s" "$0" > "$program_path/ephemeral_component.py"

          _KFP_RUNTIME=true python3 -m kfp.dsl.executor_main                         --component_module_path                         "$program_path/ephemeral_component.py"                         "$@"

          '
        - "\nimport kfp\nfrom kfp import dsl\nfrom kfp.dsl import *\nfrom typing import\
          \ *\n\ndef prepare_voc_dataset(\n    output_dataset: Output[Dataset],\n\
          \    dataset_name: str = 'voc2007'  # or 'voc2012' or 'voc0712' for both\n\
          ):\n    import os\n    import subprocess\n    from pathlib import Path\n\
          \    import shutil\n\n    # Create output directory\n    output_path = Path(output_dataset.path)\n\
          \    output_path.mkdir(parents=True, exist_ok=True)\n\n    # Create a temporary\
          \ data directory\n    data_dir = Path('./data')\n    data_dir.mkdir(exist_ok=True)\n\
          \n    print(f\"Downloading VOC dataset: {dataset_name}\")\n\n    # Download\
          \ using the MMDetection download script\n    download_script = '''\nimport\
          \ os\nimport urllib.request\nimport tarfile\nimport zipfile\nfrom pathlib\
          \ import Path\n\ndef download_voc(dataset_name, save_dir):\n    \"\"\"Download\
          \ VOC dataset\"\"\"\n    urls = {\n        'voc2007': [\n            ('http://host.robots.ox.ac.uk/pascal/VOC/voc2007/VOCtrainval_06-Nov-2007.tar',\
          \ 'VOCtrainval_06-Nov-2007.tar'),\n            ('http://host.robots.ox.ac.uk/pascal/VOC/voc2007/VOCtest_06-Nov-2007.tar',\
          \ 'VOCtest_06-Nov-2007.tar'),\n            ('http://host.robots.ox.ac.uk/pascal/VOC/voc2007/VOCdevkit_08-Jun-2007.tar',\
          \ 'VOCdevkit_08-Jun-2007.tar')\n        ],\n        'voc2012': [\n     \
          \       ('http://host.robots.ox.ac.uk/pascal/VOC/voc2012/VOCtrainval_11-May-2012.tar',\
          \ 'VOCtrainval_11-May-2012.tar')\n        ]\n    }\n\n    save_path = Path(save_dir)\n\
          \    save_path.mkdir(parents=True, exist_ok=True)\n\n    # Download files\n\
          \    if dataset_name == 'voc0712':\n        # Download both 2007 and 2012\n\
          \        download_files = urls['voc2007'] + urls['voc2012']\n    else:\n\
          \        download_files = urls.get(dataset_name, [])\n\n    for url, filename\
          \ in download_files:\n        filepath = save_path / filename\n        if\
          \ not filepath.exists():\n            print(f\"Downloading {filename}...\"\
          )\n            urllib.request.urlretrieve(url, filepath)\n\n        # Extract\n\
          \        print(f\"Extracting {filename}...\")\n        if filename.endswith('.tar'):\n\
          \            with tarfile.open(filepath, 'r') as tar:\n                tar.extractall(save_path)\n\
          \        elif filename.endswith('.zip'):\n            with zipfile.ZipFile(filepath,\
          \ 'r') as zip_ref:\n                zip_ref.extractall(save_path)\n\n  \
          \      # Remove archive after extraction\n        os.remove(filepath)\n\n\
          \    print(f\"VOC dataset downloaded to {save_path}\")\n\n# Run download\n\
          download_voc('{}', './data')\n'''.format(dataset_name)\n\n    # Save and\
          \ run the download script\n    with open('download_voc.py', 'w') as f:\n\
          \        f.write(download_script)\n\n    subprocess.run(['python', 'download_voc.py'],\
          \ check=True)\n\n    # Copy the downloaded data to output\n    voc_path\
          \ = data_dir / 'VOCdevkit'\n    if voc_path.exists():\n        shutil.copytree(voc_path,\
          \ output_path / 'VOCdevkit')\n        print(f\"Dataset copied to {output_path}\"\
          )\n    else:\n        raise RuntimeError(\"VOCdevkit not found after download!\"\
          )\n\n    # Verify dataset structure\n    print(\"Dataset structure:\")\n\
          \    for year in ['VOC2007', 'VOC2012']:\n        year_path = output_path\
          \ / 'VOCdevkit' / year\n        if year_path.exists():\n            print(f\"\
          \  {year}: \u2713\")\n            print(f\"    - Annotations: {(year_path\
          \ / 'Annotations').exists()}\")\n            print(f\"    - JPEGImages:\
          \ {(year_path / 'JPEGImages').exists()}\")\n            print(f\"    - ImageSets:\
          \ {(year_path / 'ImageSets').exists()}\")\n\n"
        image: pytorch/pytorch:2.0.1-cuda11.7-cudnn8-runtime
    exec-train-ssd-with-mlflow:
      container:
        args:
        - --executor_input
        - '{{$}}'
        - --function_to_execute
        - train_ssd_with_mlflow
        command:
        - sh
        - -c
        - "\nif ! [ -x \"$(command -v pip)\" ]; then\n    python3 -m ensurepip ||\
          \ python3 -m ensurepip --user || apt-get install python3-pip\nfi\n\nPIP_DISABLE_PIP_VERSION_CHECK=1\
          \ python3 -m pip install --quiet --no-warn-script-location 'kfp==2.9.0'\
          \ '--no-deps' 'typing-extensions>=3.7.4,<5; python_version<\"3.9\"'  &&\
          \  python3 -m pip install --quiet --no-warn-script-location 'torch==2.0.1'\
          \ 'torchvision==0.15.2' 'openmim==0.3.9' 'mmengine==0.10.7' 'mmcv==2.0.0'\
          \ 'opencv-python==4.11.0.86' 'pycocotools==2.0.7' 'lxml' 'mlflow==2.17.2'\
          \ 'pandas==2.0.3' 'matplotlib==3.7.5' 'scipy==1.10.1' 'scikit-learn==1.3.2'\
          \ 'tqdm==4.65.2' 'requests==2.28.2' && \"$0\" \"$@\"\n"
        - sh
        - -ec
        - 'program_path=$(mktemp -d)


          printf "%s" "$0" > "$program_path/ephemeral_component.py"

          _KFP_RUNTIME=true python3 -m kfp.dsl.executor_main                         --component_module_path                         "$program_path/ephemeral_component.py"                         "$@"

          '
        - "\nimport kfp\nfrom kfp import dsl\nfrom kfp.dsl import *\nfrom typing import\
          \ *\n\ndef train_ssd_with_mlflow(\n    dataset: Input[Dataset],\n    model_output:\
          \ Output[Model],\n    metrics_output: Output[Metrics],\n    config_output:\
          \ Output[Artifact],\n    github_repo: str = '',  # Your modified MMDetection\
          \ repo\n    config_path: str = 'configs/pascal_voc/ssd300_voc0712.py',\n\
          \    mlflow_tracking_uri: str = '',  # MLflow server URI if you have one\n\
          \    mlflow_experiment_name: str = 'ssd_voc_training',\n    num_epochs:\
          \ int = 24,\n    batch_size: int = 8,\n    learning_rate: float = 1e-3\n\
          ):\n    import subprocess\n    import os\n    from pathlib import Path\n\
          \    import shutil\n    import json\n    import mlflow\n    import mlflow.pytorch\n\
          \n    # Setup MLflow\n    if mlflow_tracking_uri:\n        mlflow.set_tracking_uri(mlflow_tracking_uri)\n\
          \n    mlflow.set_experiment(mlflow_experiment_name)\n\n    # Install MMDetection\n\
          \    print(\"Installing MMDetection...\")\n    subprocess.run(['mim', 'install',\
          \ 'mmdet'], check=True)\n\n    # Clone your modified repo if provided\n\
          \    if github_repo:\n        print(f\"Cloning custom repo from {github_repo}\"\
          )\n        subprocess.run(['git', 'clone', github_repo, 'mmdetection'],\
          \ check=True)\n        os.chdir('mmdetection')\n\n        # Install in editable\
          \ mode to use your modifications\n        subprocess.run(['pip', 'install',\
          \ '-e', '.'], check=True)\n\n    # Create custom config with correct data\
          \ paths\n    data_root = Path(dataset.path) / 'VOCdevkit'\n\n    custom_config\
          \ = f'''\n# Based on {config_path}\n_base_ = [\n    '../_base_/models/ssd300.py',\n\
          \    '../_base_/datasets/voc0712.py',\n    '../_base_/default_runtime.py'\n\
          ]\n\n# Data settings\ndata_root = '{data_root}/'\ndata = dict(\n    samples_per_gpu={batch_size},\n\
          \    workers_per_gpu=4,\n    train=dict(\n        _delete_=True,\n     \
          \   type='RepeatDataset',\n        times=1,\n        dataset=dict(\n   \
          \         type='VOCDataset',\n            data_root=data_root,\n       \
          \     ann_file=data_root + 'VOC2007/ImageSets/Main/trainval.txt',\n    \
          \        img_prefix=data_root + 'VOC2007/',\n            pipeline=[\n  \
          \              dict(type='LoadImageFromFile', backend_args=None),\n    \
          \            dict(type='LoadAnnotations', with_bbox=True),\n           \
          \     dict(type='Resize', img_scale=(300, 300), keep_ratio=False),\n   \
          \             dict(type='RandomFlip', flip_ratio=0.5),\n               \
          \ dict(type='PhotoMetricDistortion'),\n                dict(type='Normalize',\
          \ mean=[123.675, 116.28, 103.53], std=[1, 1, 1]),\n                dict(type='Pad',\
          \ size_divisor=1),\n                dict(type='DefaultFormatBundle'),\n\
          \                dict(type='Collect', keys=['img', 'gt_bboxes', 'gt_labels']),\n\
          \            ]\n        )\n    ),\n    val=dict(\n        type='VOCDataset',\n\
          \        data_root=data_root,\n        ann_file=data_root + 'VOC2007/ImageSets/Main/test.txt',\n\
          \        img_prefix=data_root + 'VOC2007/',\n        test_mode=True,\n \
          \       pipeline=[\n            dict(type='LoadImageFromFile', backend_args=None),\n\
          \            dict(type='Resize', img_scale=(300, 300), keep_ratio=False),\n\
          \            dict(type='Normalize', mean=[123.675, 116.28, 103.53], std=[1,\
          \ 1, 1]),\n            dict(type='Pad', size_divisor=1),\n            dict(type='ImageToTensor',\
          \ keys=['img']),\n            dict(type='Collect', keys=['img']),\n    \
          \    ]\n    ),\n    test=dict(\n        type='VOCDataset',\n        data_root=data_root,\n\
          \        ann_file=data_root + 'VOC2007/ImageSets/Main/test.txt',\n     \
          \   img_prefix=data_root + 'VOC2007/',\n        test_mode=True,\n      \
          \  pipeline=[\n            dict(type='LoadImageFromFile', backend_args=None),\n\
          \            dict(type='Resize', img_scale=(300, 300), keep_ratio=False),\n\
          \            dict(type='Normalize', mean=[123.675, 116.28, 103.53], std=[1,\
          \ 1, 1]),\n            dict(type='Pad', size_divisor=1),\n            dict(type='ImageToTensor',\
          \ keys=['img']),\n            dict(type='Collect', keys=['img']),\n    \
          \    ]\n    )\n)\n\n# Model settings\nmodel = dict(\n    bbox_head=dict(\n\
          \        num_classes=20  # VOC has 20 classes\n    )\n)\n\n# Optimizer\n\
          optimizer = dict(type='SGD', lr={learning_rate}, momentum=0.9, weight_decay=5e-4)\n\
          optimizer_config = dict()\n\n# Learning policy\nlr_config = dict(\n    policy='step',\n\
          \    warmup='linear',\n    warmup_iters=500,\n    warmup_ratio=0.001,\n\
          \    step=[16, 22])\n\n# Runtime settings\nrunner = dict(type='EpochBasedRunner',\
          \ max_epochs={num_epochs})\ncheckpoint_config = dict(interval=1)\nlog_config\
          \ = dict(\n    interval=50,\n    hooks=[\n        dict(type='TextLoggerHook'),\n\
          \        dict(type='TensorboardLoggerHook')\n    ])\n\nevaluation = dict(interval=1,\
          \ metric='mAP')\n\n# Work directory\nwork_dir = './work_dirs/ssd300_voc_kubeflow'\n\
          \n# For better performance on VOC\ncudnn_benchmark = True\n'''\n\n    #\
          \ Save custom config\n    with open('custom_ssd_config.py', 'w') as f:\n\
          \        f.write(custom_config)\n\n    # Save config for later use\n   \
          \ shutil.copy('custom_ssd_config.py', config_output.path)\n\n    # Start\
          \ MLflow run\n    with mlflow.start_run() as run:\n        # Log parameters\n\
          \        mlflow.log_param(\"model_type\", \"ssd300\")\n        mlflow.log_param(\"\
          dataset\", \"voc2007\")  # Changed to voc2007\n        mlflow.log_param(\"\
          num_epochs\", num_epochs)\n        mlflow.log_param(\"batch_size\", batch_size)\n\
          \        mlflow.log_param(\"learning_rate\", learning_rate)\n        mlflow.log_param(\"\
          optimizer\", \"SGD\")\n\n        # Run training\n        print(\"Starting\
          \ training with MLflow tracking...\")\n\n        # If you have your modified\
          \ train.py with MLflow\n        if github_repo or os.path.exists('tools/train.py'):\n\
          \            # Use your train.py which should have MLflow integration\n\
          \            cmd = [\n                'python', 'tools/train.py',\n    \
          \            'custom_ssd_config.py',\n                '--work-dir', './work_dirs/ssd300_voc_kubeflow'\n\
          \            ]\n            result = subprocess.run(cmd, capture_output=True,\
          \ text=True)\n            print(result.stdout)\n            if result.returncode\
          \ != 0:\n                print(f\"Error: {result.stderr}\")\n          \
          \      raise RuntimeError(f\"Training failed: {result.stderr}\")\n\n   \
          \     # Find and save the best model\n        work_dir = Path('./work_dirs/ssd300_voc_kubeflow')\n\
          \        checkpoints = list(work_dir.glob('epoch_*.pth'))\n\n        if\
          \ checkpoints:\n            # Get the latest checkpoint\n            latest_checkpoint\
          \ = sorted(checkpoints, key=lambda x: int(x.stem.split('_')[1]))[-1]\n\n\
          \            # Copy model to output\n            shutil.copy(latest_checkpoint,\
          \ model_output.path)\n            print(f\"Model saved to {model_output.path}\"\
          )\n\n            # Log model to MLflow\n            mlflow.pytorch.log_model(\n\
          \                pytorch_model=model_output.path,\n                artifact_path=\"\
          model\",\n                registered_model_name=\"ssd300_voc2007\"  # Changed\
          \ name\n            )\n        else:\n            raise RuntimeError(\"\
          No checkpoint found after training!\")\n\n        # Log final metrics\n\
          \        metrics_output.log_metric('epochs_trained', num_epochs)\n     \
          \   print(f\"Training completed! MLflow run ID: {run.info.run_id}\")\n\n"
        image: pytorch/pytorch:2.0.1-cuda11.7-cudnn8-runtime
pipelineInfo:
  description: Train SSD300 on Pascal VOC 2007 with MLflow tracking
  name: mmdetection-ssd-voc2007-training-with-mlflow
root:
  dag:
    outputs:
      artifacts:
        train-ssd-with-mlflow-metrics_output:
          artifactSelectors:
          - outputArtifactKey: metrics_output
            producerSubtask: train-ssd-with-mlflow
    tasks:
      prepare-voc-dataset:
        cachingOptions:
          enableCache: true
        componentRef:
          name: comp-prepare-voc-dataset
        inputs:
          parameters:
            dataset_name:
              runtimeValue:
                constant: voc2007
        taskInfo:
          name: prepare-voc-dataset
      train-ssd-with-mlflow:
        cachingOptions:
          enableCache: true
        componentRef:
          name: comp-train-ssd-with-mlflow
        dependentTasks:
        - prepare-voc-dataset
        inputs:
          artifacts:
            dataset:
              taskOutputArtifact:
                outputArtifactKey: output_dataset
                producerTask: prepare-voc-dataset
          parameters:
            batch_size:
              componentInputParameter: batch_size
            config_path:
              runtimeValue:
                constant: configs/pascal_voc/ssd300_voc0712.py
            github_repo:
              componentInputParameter: github_repo
            learning_rate:
              componentInputParameter: learning_rate
            mlflow_experiment_name:
              componentInputParameter: mlflow_experiment_name
            mlflow_tracking_uri:
              componentInputParameter: mlflow_tracking_uri
            num_epochs:
              componentInputParameter: num_epochs
        taskInfo:
          name: train-ssd-with-mlflow
  inputDefinitions:
    parameters:
      batch_size:
        defaultValue: 8.0
        isOptional: true
        parameterType: NUMBER_INTEGER
      github_repo:
        defaultValue: ''
        isOptional: true
        parameterType: STRING
      learning_rate:
        defaultValue: 0.001
        isOptional: true
        parameterType: NUMBER_DOUBLE
      mlflow_experiment_name:
        defaultValue: ssd_voc_training
        isOptional: true
        parameterType: STRING
      mlflow_tracking_uri:
        defaultValue: ''
        isOptional: true
        parameterType: STRING
      num_epochs:
        defaultValue: 24.0
        isOptional: true
        parameterType: NUMBER_INTEGER
  outputDefinitions:
    artifacts:
      train-ssd-with-mlflow-metrics_output:
        artifactType:
          schemaTitle: system.Metrics
          schemaVersion: 0.0.1
schemaVersion: 2.1.0
sdkVersion: kfp-2.9.0
